{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASETS Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data_files = {'train': 'data/drugsComTrain_raw.tsv', 'test': 'data/drugsComTest_raw.tsv'}\n",
    "drug_dataset = load_dataset('csv', data_files=data_files, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (161297, 7), 'test': (53766, 7)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
       " 'drugName': Value(dtype='string', id=None),\n",
       " 'condition': Value(dtype='string', id=None),\n",
       " 'review': Value(dtype='string', id=None),\n",
       " 'rating': Value(dtype='float64', id=None),\n",
       " 'date': Value(dtype='string', id=None),\n",
       " 'usefulCount': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['Left Ventricular Dysfunction', 'ADHD', 'Birth Control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique('Unnamed: 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(original_column_name='Unnamed: 0', new_column_name='patient_id')\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {'condition': example['condition'].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['Left Ventricular Dysfunction', 'ADHD', 'Birth Control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nones(x):\n",
    "    return x['condition'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x['condition'] is not None)\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['left ventricular dysfunction', 'adhd', 'birth control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['review'].split())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "drug_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['hepatitis c', 'adhd', 'birth control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'].sort('review_length')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x['review_length'] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "drug_dataset = drug_dataset.map(lambda x: {'review': [html.unescape(o) for o in x['review']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 138514, 'test': 46108}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46108/46108 [00:11<00:00, 4103.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 515 ms, total: 16.5 s\n",
      "Wall time: 11.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 ms, sys: 4.08 ms, total: 23.1 ms\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [02:00<00:00, 1149.17 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:39<00:00, 1160.32 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 919 ms, total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=False)\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples['review'], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [02:35<00:00, 890.88 examples/s] \n",
      "Map: 100%|██████████| 46108/46108 [00:51<00:00, 886.76 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 3.21 s, total: 3min 28s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 95260,\n",
       " 'drugName': 'Guanfacine',\n",
       " 'condition': 'adhd',\n",
       " 'review': '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       " 'rating': 8.0,\n",
       " 'date': 'April 27, 2010',\n",
       " 'usefulCount': 192,\n",
       " 'review_length': 141}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 49]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "[len(inp) for inp in result[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [95260, 92703, 138000],\n",
       " 'drugName': ['Guanfacine', 'Lybrel', 'Ortho Evra'],\n",
       " 'condition': ['adhd', 'birth control', 'birth control'],\n",
       " 'review': ['\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
       "  '\"This is my first time using any form of birth control. I\\'m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"'],\n",
       " 'rating': [8.0, 5.0, 8.0],\n",
       " 'date': ['April 27, 2010', 'December 14, 2009', 'November 3, 2015'],\n",
       " 'usefulCount': [192, 17, 10],\n",
       " 'review_length': [141, 134, 89]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    drugName      condition  \\\n",
       "0       95260  Guanfacine           adhd   \n",
       "1       92703      Lybrel  birth control   \n",
       "2      138000  Ortho Evra  birth control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.set_format('pandas')\n",
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('train', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 110811\n",
       "})), ('validation', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 27703\n",
       "})), ('test', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 46108\n",
       "}))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 26.0MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 12.2M/12.2M [00:00<00:00, 12.9MB/s]\n",
      "Generating train split: 100%|██████████| 3019/3019 [00:00<00:00, 16763.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "issue_dataset = load_dataset('lewtun/github-issues', split='train')\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repository_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>html_url</th>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2955</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2954</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1003999469</td>\n",
       "      <td>1003904803</td>\n",
       "      <td>1002704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <td>PR_kwDODunzps4sHuRu</td>\n",
       "      <td>PR_kwDODunzps4sHa8O</td>\n",
       "      <td>PR_kwDODunzps4sDU8S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>2955</td>\n",
       "      <td>2954</td>\n",
       "      <td>2952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Update legacy Python image for CI tests in Linux</td>\n",
       "      <td>Run tests in parallel</td>\n",
       "      <td>Fix missing conda deps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>{'login': 'albertvillanova', 'id': 8515462, 'n...</td>\n",
       "      <td>{'login': 'albertvillanova', 'id': 8515462, 'n...</td>\n",
       "      <td>{'login': 'lhoestq', 'id': 42851186, 'node_id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>open</td>\n",
       "      <td>open</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locked</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignee</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignees</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milestone</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments</th>\n",
       "      <td>[]</td>\n",
       "      <td>[There is a speed up in Windows machines:\\r\\n-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>1632299127000</td>\n",
       "      <td>1632294044000</td>\n",
       "      <td>1632237781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>updated_at</th>\n",
       "      <td>1632302608000</td>\n",
       "      <td>1632297373000</td>\n",
       "      <td>1632285599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closed_at</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1632238244000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_association</th>\n",
       "      <td>MEMBER</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>MEMBER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_lock_reason</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pull_request</th>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>Instead of legacy, use next-generation conveni...</td>\n",
       "      <td>Run CI tests in parallel to speed up the test ...</td>\n",
       "      <td>`aiohttp` was added as a dependency in #2662 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeline_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_pull_request</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          0  ...                                                  2\n",
       "url                       https://api.github.com/repos/huggingface/datas...  ...  https://api.github.com/repos/huggingface/datas...\n",
       "repository_url            https://api.github.com/repos/huggingface/datasets  ...  https://api.github.com/repos/huggingface/datasets\n",
       "labels_url                https://api.github.com/repos/huggingface/datas...  ...  https://api.github.com/repos/huggingface/datas...\n",
       "comments_url              https://api.github.com/repos/huggingface/datas...  ...  https://api.github.com/repos/huggingface/datas...\n",
       "events_url                https://api.github.com/repos/huggingface/datas...  ...  https://api.github.com/repos/huggingface/datas...\n",
       "html_url                  https://github.com/huggingface/datasets/pull/2955  ...  https://github.com/huggingface/datasets/pull/2952\n",
       "id                                                               1003999469  ...                                         1002704096\n",
       "node_id                                                 PR_kwDODunzps4sHuRu  ...                                PR_kwDODunzps4sDU8S\n",
       "number                                                                 2955  ...                                               2952\n",
       "title                      Update legacy Python image for CI tests in Linux  ...                             Fix missing conda deps\n",
       "user                      {'login': 'albertvillanova', 'id': 8515462, 'n...  ...  {'login': 'lhoestq', 'id': 42851186, 'node_id'...\n",
       "labels                                                                   []  ...                                                 []\n",
       "state                                                                  open  ...                                             closed\n",
       "locked                                                                False  ...                                              False\n",
       "assignee                                                               None  ...                                               None\n",
       "assignees                                                                []  ...                                                 []\n",
       "milestone                                                              None  ...                                               None\n",
       "comments                                                                 []  ...                                                 []\n",
       "created_at                                                    1632299127000  ...                                      1632237781000\n",
       "updated_at                                                    1632302608000  ...                                      1632285599000\n",
       "closed_at                                                               NaN  ...                                    1632238244000.0\n",
       "author_association                                                   MEMBER  ...                                             MEMBER\n",
       "active_lock_reason                                                     None  ...                                               None\n",
       "pull_request              {'url': 'https://api.github.com/repos/huggingf...  ...  {'url': 'https://api.github.com/repos/huggingf...\n",
       "body                      Instead of legacy, use next-generation conveni...  ...  `aiohttp` was added as a dependency in #2662 b...\n",
       "timeline_url              https://api.github.com/repos/huggingface/datas...  ...  https://api.github.com/repos/huggingface/datas...\n",
       "performed_via_github_app                                               None  ...                                               None\n",
       "is_pull_request                                                        True  ...                                               True\n",
       "\n",
       "[28 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(issue_dataset)[:3].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3019/3019 [00:00<00:00, 11618.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_dataset = issue_dataset.filter(lambda x: x['is_pull_request'] == False and len(x['comments']) > 0)\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issue_dataset.column_names\n",
    "columns_to_keep = ['title', 'body', 'html_url', 'comments']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issue_dataset = issue_dataset.remove_columns(columns_to_remove)\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  ...                                               body\n",
       "0  https://github.com/huggingface/datasets/issues...  ...  After accidental merge commit (91c55355b634d0d...\n",
       "1  https://github.com/huggingface/datasets/issues...  ...  ## Describe the bug\\r\\nAfter upgrading to data...\n",
       "2  https://github.com/huggingface/datasets/issues...  ...  ## Describe the bug\\r\\n\\r\\nCannot download OSC...\n",
       "\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(issue_dataset)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  ...                                               body\n",
       "0  https://github.com/huggingface/datasets/issues...  ...  After accidental merge commit (91c55355b634d0d...\n",
       "1  https://github.com/huggingface/datasets/issues...  ...  ## Describe the bug\\r\\nAfter upgrading to data...\n",
       "2  https://github.com/huggingface/datasets/issues...  ...  ## Describe the bug\\r\\n\\r\\nCannot download OSC...\n",
       "3  https://github.com/huggingface/datasets/issues...  ...  ## Describe the bug\\r\\nStandard process to dow...\n",
       "4  https://github.com/huggingface/datasets/issues...  ...  To reproduce:\\r\\n```python\\r\\nimport datasets ...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_dataset.set_format('pandas')\n",
    "df = issue_dataset[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>html_url</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments</th>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0  ...                                                  2\n",
       "html_url  https://github.com/huggingface/datasets/issues...  ...  https://github.com/huggingface/datasets/issues...\n",
       "title                                 Protect master branch  ...  Backwards compatibility broken for cached data...\n",
       "comments                    Cool, I think we can do both :)  ...  Hi ! I guess the caching mechanism should have...\n",
       "body      After accidental merge commit (91c55355b634d0d...  ...  ## Describe the bug\\r\\nAfter upgrading to data...\n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode('comments', ignore_index=True)\n",
    "comments_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2964/2964 [00:00<00:00, 14019.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {'comment_length': len(x['comments'].split())})\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2964/2964 [00:00<00:00, 71612.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x['comment_length'] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': ['https://github.com/huggingface/datasets/issues/2945',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943'],\n",
       " 'title': ['Protect master branch',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`'],\n",
       " 'comments': ['@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       "  \"Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?\",\n",
       "  \"If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests.\",\n",
       "  \"Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR\",\n",
       "  \"I just merged a fix, let me know if you're still having this kind of issues :)\\r\\n\\r\\nWe'll do a release soon to make this fix available\"],\n",
       " 'body': ['After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n'],\n",
       " 'comment_length': [64, 50, 28, 22, 27]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(example):\n",
    "    return {'text':example['title'] + ' \\n' + example['body'] + ' \\n' + example['comments']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2175/2175 [00:00<00:00, 8001.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 64,\n",
       " 'text': 'Protect master branch \\nAfter accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_ckpt = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.detach().cpu().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 1/2175 [00:00<24:53,  1.46 examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1672) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcomments_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/datasets/arrow_dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3515\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3517\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings_dataset \u001b[38;5;241m=\u001b[39m comments_dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]})\n",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text_list, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encoded_input\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 4\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cls_pooling(model_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1077\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m-> 1077\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:216\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    215\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 216\u001b[0m     \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[1;32m    217\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    218\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1672) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(lambda x: {'embeddings': get_embeddings(x['text']).detach().cpu().numpy()[0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
