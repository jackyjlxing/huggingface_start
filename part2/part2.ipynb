{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASETS Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data_files = {'train': 'data/drugsComTrain_raw.tsv', 'test': 'data/drugsComTest_raw.tsv'}\n",
    "drug_dataset = load_dataset('csv', data_files=data_files, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (161297, 7), 'test': (53766, 7)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
       " 'drugName': Value(dtype='string', id=None),\n",
       " 'condition': Value(dtype='string', id=None),\n",
       " 'review': Value(dtype='string', id=None),\n",
       " 'rating': Value(dtype='float64', id=None),\n",
       " 'date': Value(dtype='string', id=None),\n",
       " 'usefulCount': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['Left Ventricular Dysfunction', 'ADHD', 'Birth Control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique('Unnamed: 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(original_column_name='Unnamed: 0', new_column_name='patient_id')\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {'condition': example['condition'].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['Left Ventricular Dysfunction', 'ADHD', 'Birth Control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nones(x):\n",
    "    return x['condition'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/161297 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 161297/161297 [00:01<00:00, 149286.04 examples/s]\n",
      "Filter: 100%|██████████| 53766/53766 [00:00<00:00, 165832.36 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x['condition'] is not None)\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [00:18<00:00, 8488.32 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:05<00:00, 9056.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': [206461, 95260, 92703],\n",
       " 'drugName': ['Valsartan', 'Guanfacine', 'Lybrel'],\n",
       " 'condition': ['left ventricular dysfunction', 'adhd', 'birth control'],\n",
       " 'review': ['\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"'],\n",
       " 'rating': [9.0, 8.0, 5.0],\n",
       " 'date': ['May 20, 2012', 'April 27, 2010', 'December 14, 2009'],\n",
       " 'usefulCount': [27, 192, 17]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['review'].split())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [00:13<00:00, 11849.69 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:04<00:00, 11929.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "drug_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['hepatitis c', 'adhd', 'birth control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'].sort('review_length')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 160398/160398 [00:01<00:00, 142847.01 examples/s]\n",
      "Filter: 100%|██████████| 53471/53471 [00:00<00:00, 149399.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x['review_length'] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [00:07<00:00, 19577.39 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:02<00:00, 20529.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "drug_dataset = drug_dataset.map(lambda x: {'review': [html.unescape(o) for o in x['review']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 138514, 'test': 46108}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [00:22<00:00, 6194.49 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:07<00:00, 6506.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1.61 s, total: 1min 4s\n",
      "Wall time: 29.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [01:15<00:00, 1844.71 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:24<00:00, 1846.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 672 ms, total: 1min 39s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [02:02<00:00, 1126.22 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:40<00:00, 1129.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 545 ms, total: 2min 44s\n",
      "Wall time: 2min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=False)\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples['review'], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [02:30<00:00, 922.05 examples/s] \n",
      "Map: 100%|██████████| 46108/46108 [00:49<00:00, 924.77 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 18s, sys: 3.39 s, total: 3min 21s\n",
      "Wall time: 3min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 95260,\n",
       " 'drugName': 'Guanfacine',\n",
       " 'condition': 'adhd',\n",
       " 'review': '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       " 'rating': 8.0,\n",
       " 'date': 'April 27, 2010',\n",
       " 'usefulCount': 192,\n",
       " 'review_length': 141}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 49]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "[len(inp) for inp in result[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [95260, 92703, 138000],\n",
       " 'drugName': ['Guanfacine', 'Lybrel', 'Ortho Evra'],\n",
       " 'condition': ['adhd', 'birth control', 'birth control'],\n",
       " 'review': ['\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
       "  '\"This is my first time using any form of birth control. I\\'m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"'],\n",
       " 'rating': [8.0, 5.0, 8.0],\n",
       " 'date': ['April 27, 2010', 'December 14, 2009', 'November 3, 2015'],\n",
       " 'usefulCount': [192, 17, 10],\n",
       " 'review_length': [141, 134, 89]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    drugName      condition  \\\n",
       "0       95260  Guanfacine           adhd   \n",
       "1       92703      Lybrel  birth control   \n",
       "2      138000  Ortho Evra  birth control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.set_format('pandas')\n",
    "drug_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('train', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 110811\n",
       "})), ('validation', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 27703\n",
       "})), ('test', Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 46108\n",
       "}))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "issue_dataset = load_dataset('lewtun/github-issues', split='train')\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repository_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>html_url</th>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2955</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2954</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/2952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1003999469</td>\n",
       "      <td>1003904803</td>\n",
       "      <td>1002704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <td>PR_kwDODunzps4sHuRu</td>\n",
       "      <td>PR_kwDODunzps4sHa8O</td>\n",
       "      <td>PR_kwDODunzps4sDU8S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>2955</td>\n",
       "      <td>2954</td>\n",
       "      <td>2952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Update legacy Python image for CI tests in Linux</td>\n",
       "      <td>Run tests in parallel</td>\n",
       "      <td>Fix missing conda deps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>{'login': 'albertvillanova', 'id': 8515462, 'n...</td>\n",
       "      <td>{'login': 'albertvillanova', 'id': 8515462, 'n...</td>\n",
       "      <td>{'login': 'lhoestq', 'id': 42851186, 'node_id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>open</td>\n",
       "      <td>open</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locked</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignee</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignees</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milestone</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments</th>\n",
       "      <td>[]</td>\n",
       "      <td>[There is a speed up in Windows machines:\\r\\n-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>1632299127000</td>\n",
       "      <td>1632294044000</td>\n",
       "      <td>1632237781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>updated_at</th>\n",
       "      <td>1632302608000</td>\n",
       "      <td>1632297373000</td>\n",
       "      <td>1632285599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closed_at</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1632238244000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_association</th>\n",
       "      <td>MEMBER</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>MEMBER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_lock_reason</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pull_request</th>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>Instead of legacy, use next-generation conveni...</td>\n",
       "      <td>Run CI tests in parallel to speed up the test ...</td>\n",
       "      <td>`aiohttp` was added as a dependency in #2662 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeline_url</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_pull_request</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          0  \\\n",
       "url                       https://api.github.com/repos/huggingface/datas...   \n",
       "repository_url            https://api.github.com/repos/huggingface/datasets   \n",
       "labels_url                https://api.github.com/repos/huggingface/datas...   \n",
       "comments_url              https://api.github.com/repos/huggingface/datas...   \n",
       "events_url                https://api.github.com/repos/huggingface/datas...   \n",
       "html_url                  https://github.com/huggingface/datasets/pull/2955   \n",
       "id                                                               1003999469   \n",
       "node_id                                                 PR_kwDODunzps4sHuRu   \n",
       "number                                                                 2955   \n",
       "title                      Update legacy Python image for CI tests in Linux   \n",
       "user                      {'login': 'albertvillanova', 'id': 8515462, 'n...   \n",
       "labels                                                                   []   \n",
       "state                                                                  open   \n",
       "locked                                                                False   \n",
       "assignee                                                               None   \n",
       "assignees                                                                []   \n",
       "milestone                                                              None   \n",
       "comments                                                                 []   \n",
       "created_at                                                    1632299127000   \n",
       "updated_at                                                    1632302608000   \n",
       "closed_at                                                               NaN   \n",
       "author_association                                                   MEMBER   \n",
       "active_lock_reason                                                     None   \n",
       "pull_request              {'url': 'https://api.github.com/repos/huggingf...   \n",
       "body                      Instead of legacy, use next-generation conveni...   \n",
       "timeline_url              https://api.github.com/repos/huggingface/datas...   \n",
       "performed_via_github_app                                               None   \n",
       "is_pull_request                                                        True   \n",
       "\n",
       "                                                                          1  \\\n",
       "url                       https://api.github.com/repos/huggingface/datas...   \n",
       "repository_url            https://api.github.com/repos/huggingface/datasets   \n",
       "labels_url                https://api.github.com/repos/huggingface/datas...   \n",
       "comments_url              https://api.github.com/repos/huggingface/datas...   \n",
       "events_url                https://api.github.com/repos/huggingface/datas...   \n",
       "html_url                  https://github.com/huggingface/datasets/pull/2954   \n",
       "id                                                               1003904803   \n",
       "node_id                                                 PR_kwDODunzps4sHa8O   \n",
       "number                                                                 2954   \n",
       "title                                                 Run tests in parallel   \n",
       "user                      {'login': 'albertvillanova', 'id': 8515462, 'n...   \n",
       "labels                                                                   []   \n",
       "state                                                                  open   \n",
       "locked                                                                False   \n",
       "assignee                                                               None   \n",
       "assignees                                                                []   \n",
       "milestone                                                              None   \n",
       "comments                  [There is a speed up in Windows machines:\\r\\n-...   \n",
       "created_at                                                    1632294044000   \n",
       "updated_at                                                    1632297373000   \n",
       "closed_at                                                               NaN   \n",
       "author_association                                                   MEMBER   \n",
       "active_lock_reason                                                     None   \n",
       "pull_request              {'url': 'https://api.github.com/repos/huggingf...   \n",
       "body                      Run CI tests in parallel to speed up the test ...   \n",
       "timeline_url              https://api.github.com/repos/huggingface/datas...   \n",
       "performed_via_github_app                                               None   \n",
       "is_pull_request                                                        True   \n",
       "\n",
       "                                                                          2  \n",
       "url                       https://api.github.com/repos/huggingface/datas...  \n",
       "repository_url            https://api.github.com/repos/huggingface/datasets  \n",
       "labels_url                https://api.github.com/repos/huggingface/datas...  \n",
       "comments_url              https://api.github.com/repos/huggingface/datas...  \n",
       "events_url                https://api.github.com/repos/huggingface/datas...  \n",
       "html_url                  https://github.com/huggingface/datasets/pull/2952  \n",
       "id                                                               1002704096  \n",
       "node_id                                                 PR_kwDODunzps4sDU8S  \n",
       "number                                                                 2952  \n",
       "title                                                Fix missing conda deps  \n",
       "user                      {'login': 'lhoestq', 'id': 42851186, 'node_id'...  \n",
       "labels                                                                   []  \n",
       "state                                                                closed  \n",
       "locked                                                                False  \n",
       "assignee                                                               None  \n",
       "assignees                                                                []  \n",
       "milestone                                                              None  \n",
       "comments                                                                 []  \n",
       "created_at                                                    1632237781000  \n",
       "updated_at                                                    1632285599000  \n",
       "closed_at                                                   1632238244000.0  \n",
       "author_association                                                   MEMBER  \n",
       "active_lock_reason                                                     None  \n",
       "pull_request              {'url': 'https://api.github.com/repos/huggingf...  \n",
       "body                      `aiohttp` was added as a dependency in #2662 b...  \n",
       "timeline_url              https://api.github.com/repos/huggingface/datas...  \n",
       "performed_via_github_app                                               None  \n",
       "is_pull_request                                                        True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(issue_dataset)[:3].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3019/3019 [00:00<00:00, 12771.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_dataset = issue_dataset.filter(lambda x: x['is_pull_request'] == False and len(x['comments']) > 0)\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issue_dataset.column_names\n",
    "columns_to_keep = ['title', 'body', 'html_url', 'comments']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issue_dataset = issue_dataset.remove_columns(columns_to_remove)\n",
    "issue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1  Backwards compatibility broken for cached data...   \n",
       "2  OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Cool, I think we can do both :), @lhoestq now...   \n",
       "1  [Hi ! I guess the caching mechanism should hav...   \n",
       "2  [I tried `unshuffled_original_da` and it is al...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSC...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(issue_dataset)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1  Backwards compatibility broken for cached data...   \n",
       "2  OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "3  load_dataset using default cache on Windows ca...   \n",
       "4  to_tf_dataset keeps a reference to the open da...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Cool, I think we can do both :), @lhoestq now...   \n",
       "1  [Hi ! I guess the caching mechanism should hav...   \n",
       "2  [I tried `unshuffled_original_da` and it is al...   \n",
       "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...   \n",
       "4  [I did some investigation and, as it seems, th...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "3  ## Describe the bug\\r\\nStandard process to dow...  \n",
       "4  To reproduce:\\r\\n```python\\r\\nimport datasets ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_dataset.set_format('pandas')\n",
    "df = issue_dataset[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>html_url</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments</th>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0  \\\n",
       "html_url  https://github.com/huggingface/datasets/issues...   \n",
       "title                                 Protect master branch   \n",
       "comments                    Cool, I think we can do both :)   \n",
       "body      After accidental merge commit (91c55355b634d0d...   \n",
       "\n",
       "                                                          1  \\\n",
       "html_url  https://github.com/huggingface/datasets/issues...   \n",
       "title                                 Protect master branch   \n",
       "comments  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "body      After accidental merge commit (91c55355b634d0d...   \n",
       "\n",
       "                                                          2  \n",
       "html_url  https://github.com/huggingface/datasets/issues...  \n",
       "title     Backwards compatibility broken for cached data...  \n",
       "comments  Hi ! I guess the caching mechanism should have...  \n",
       "body      ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode('comments', ignore_index=True)\n",
    "comments_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2964/2964 [00:00<00:00, 11278.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {'comment_length': len(x['comments'].split())})\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2964/2964 [00:00<00:00, 120654.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x['comment_length'] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': ['https://github.com/huggingface/datasets/issues/2945',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943',\n",
       "  'https://github.com/huggingface/datasets/issues/2943'],\n",
       " 'title': ['Protect master branch',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`'],\n",
       " 'comments': ['@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       "  \"Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?\",\n",
       "  \"If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests.\",\n",
       "  \"Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR\",\n",
       "  \"I just merged a fix, let me know if you're still having this kind of issues :)\\r\\n\\r\\nWe'll do a release soon to make this fix available\"],\n",
       " 'body': ['After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n'],\n",
       " 'comment_length': [64, 50, 28, 22, 27]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(example):\n",
    "    return {'text':example['title'] + ' \\n' + example['body'] + ' \\n' + example['comments']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2175/2175 [00:00<00:00, 8426.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 64,\n",
       " 'text': 'Protect master branch \\nAfter accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/anaconda3/envs/huggingface/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_ckpt = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True,max_length=512, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    # print(f'encoded_input size : {encoded_input['input_ids'].size()}')\n",
    "    model_output = model(**encoded_input)\n",
    "    # print(f'model_output last hidden state size : {model_output.last_hidden_state.size()}')\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.detach().cpu().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2175/2175 [18:14<00:00,  1.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(lambda x: {'embeddings': get_embeddings(x['text']).detach().numpy()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 268.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column='embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings(question).detach().cpu().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples('embeddings', question_embedding, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dict)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scores),type(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df['scores'] = scores\n",
    "samples_df.sort_values('scores', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "COMMENT: There are already a few transformations that you can apply on a dataset using methods like `dataset.map()`.\n",
      "You can find examples in the documentation here:\n",
      "https://huggingface.co/docs/datasets/processing.html\n",
      "\n",
      "You can merge two datasets with `concatenate_datasets()` or do label extraction with `dataset.map()` for example\n",
      "SCORE: 203.49624633789062\n",
      "TITLE: Transformer Class on dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2596\n",
      "==================================================\n",
      "\n",
      "3\n",
      "COMMENT: I'm not sure I understand your issue, can you elaborate ?\n",
      "\n",
      "`cache_file_name` is indeed an argument you can set to specify the cache file that will be used for the processed dataset. By default the file is named with something like `cache-<fingerprint>.arrow` where the fingerprint is a hash.\n",
      "SCORE: 203.44866943359375\n",
      "TITLE: is there a way to override a dataset object saved with save_to_disk?\n",
      "URL: https://github.com/huggingface/datasets/issues/2055\n",
      "==================================================\n",
      "\n",
      "2\n",
      "COMMENT: @lhoestq How can we make sure that the data we upload on HuggingFace hub is available in form of preprocessed arrow files ?\n",
      "SCORE: 197.76681518554688\n",
      "TITLE: How to disable making arrow tables in load_dataset ?\n",
      "URL: https://github.com/huggingface/datasets/issues/2092\n",
      "==================================================\n",
      "\n",
      "1\n",
      "COMMENT: From the description given in the dataset script for `wsc.fixed`:\n",
      "```\n",
      "This version fixes issues where the spans are not actually substrings of the text.\n",
      "```\n",
      "SCORE: 195.4368896484375\n",
      "TITLE: difference between wsc and wsc.fixed for superglue\n",
      "URL: https://github.com/huggingface/datasets/issues/1745\n",
      "==================================================\n",
      "\n",
      "0\n",
      "COMMENT: Hi ! Do you have an example in mind that shows how this could be useful ?\n",
      "SCORE: 176.09649658203125\n",
      "TITLE: Transformer Class on dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2596\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(_)\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
